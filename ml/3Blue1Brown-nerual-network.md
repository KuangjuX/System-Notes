# 神经网络

## 简单的概念

最简单的例子：手写数字识别

输入：一个手写数字的所有像素点；输出：数字从 0-10 的概率。

从一个向量到另一个向量的输出，本质是一个函数。

最简单的全连接神经网络，中间有不同的激活层，上一层的所有神经单元（也就是向量中的一个数）都连接到下一个激活层的神经单元上，连接上有一个权重。

为了保证保证每个神经元值都在 0 - 1 之间，需要外面包一层函数进行映射:

$$
\omega(w_1a + w_2a + ... w_na + b)
$$

可以写成：

$$
\omega(\begin{pmatrix}
w_1\\
w_2\\
..\\
w_n
\end{pmatrix}* a + b)
$$

$$
\begin{pmatrix}
w_1\\
w_2\\
..\\
w_n
\end{pmatrix}
$$

是权重矩阵，a 是输入向量，b 是偏置值向量，用来标是神经单元是否被容易激活；$ \omega $ 是映射函数，可以用 $Sigmod$  或者 $ReLU$。

## 如何进行训练

使用“代价函数”告诉机器出错了，也就是损失函数（用最终的输出 - 想要的输出的平方和加起来）。

损失函数：输入为所有参数，包括权重和偏置值，输出为一个数，用来表示这个网络有多差，输出越大网络越差。神经网络的实质就是让损失函数的值最小。

**如何使损失函数下降最快？**

计算权重向量的负梯度向量（函数下降最快的方向）。计算梯度算法是神经网络的核心，也就是反向传播算法。通过梯度下降算法用来调整神经网络的权重参数，从而继续更新网络使之损失函数达到最小。

## 反向传播算法

**问题：隐含层的含义是什么？似乎并没有真实的意义？**

在输入的向量中，每个神经元表示像素灰度的亮度，值从 0 - 1；输出为 10 个数字，值分别为 0 - 1，分别表示该图像是某个数字的可能性。

例子中有两个隐藏层，每个隐藏层有 16 个神经单元。隐藏层可以表示某种特殊的图形，例如，“8” 是上面一个圈，下面一个圈，“9” 是上面一个圈，下面一竖。



输出的每个神经元都对参数有期待，需要把每个输出神经元的期待相加起来进行反向传播。这样需要算所有的梯度下降并计算平均值，这样的计算时间非常长。可以取 minibatch 用于计算梯度下降，也叫做**随机梯度下降**。




